import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import mysql.connector
import os
from dotenv import load_dotenv
import json

print("ğŸ” æµ‹è¯•æ–°çš„NHK Easy Newsç»“æ„...")

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv('backend/.env')

try:
    # 1. è·å–NHKä¸»é¡µ
    response = requests.get('https://www3.nhk.or.jp/news/easy/', timeout=30)
    print(f"ä¸»é¡µçŠ¶æ€ç : {response.status_code}")
    
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # 2. æŸ¥æ‰¾æ–‡ç« é“¾æ¥ï¼ˆæ–°çš„æ¨¡å¼ï¼‰
    article_links = []
    for link in soup.find_all('a', href=True):
        href = link.get('href')
        if href and 'article' in href and href.startswith('./article/'):
            full_url = urljoin('https://www3.nhk.or.jp/news/easy/', href)
            article_links.append(full_url)
    
    print(f"æ‰¾åˆ° {len(article_links)} ä¸ªæ–‡ç« é“¾æ¥")
    
    # 3. æµ‹è¯•æŠ“å–å‰3ç¯‡æ–‡ç« 
    scraped_articles = []
    for i, url in enumerate(article_links[:3]):
        print(f"\nğŸ“– æŠ“å–æ–‡ç«  {i+1}: {url}")
        
        try:
            article_response = requests.get(url, timeout=30)
            article_response.raise_for_status()
            
            article_soup = BeautifulSoup(article_response.content, 'html.parser')
            
            # æå–æ ‡é¢˜
            title = article_soup.find('title')
            title_text = title.get_text(strip=True) if title else "æœªçŸ¥æ ‡é¢˜"
            
            # æå–å†…å®¹
            content = ""
            
            # å°è¯•å¤šç§å†…å®¹é€‰æ‹©å™¨
            content_selectors = [
                'div[class*="article"]',
                'div[class*="content"]',
                'main',
                'article',
                '.content',
                'body'
            ]
            
            for selector in content_selectors:
                elements = article_soup.select(selector)
                if elements:
                    content = ' '.join([elem.get_text(strip=True) for elem in elements])
                    if len(content) > 100:
                        break
            
            # å¦‚æœè¿˜æ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿå†…å®¹ï¼Œæå–æ•´ä¸ªbody
            if len(content) < 100:
                body = article_soup.find('body')
                if body:
                    content = body.get_text(strip=True)
            
            print(f"   æ ‡é¢˜: {title_text}")
            print(f"   å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            print(f"   å†…å®¹é¢„è§ˆ: {content[:150]}...")
            
            if len(content) > 50:
                scraped_articles.append({
                    'url': url,
                    'title': title_text,
                    'content': content,
                    'source': 'NHK Easy News',
                    'type': 'course'
                })
                print("   âœ… æŠ“å–æˆåŠŸ")
            else:
                print("   âŒ å†…å®¹å¤ªå°‘")
                
        except Exception as e:
            print(f"   âŒ æŠ“å–å¤±è´¥: {e}")
    
    print(f"\nğŸ“Š æˆåŠŸæŠ“å– {len(scraped_articles)} ç¯‡æ–‡ç« ")
    
    # 4. æµ‹è¯•ä¿å­˜åˆ°æ•°æ®åº“
    if scraped_articles:
        print("\nğŸ’¾ æµ‹è¯•ä¿å­˜åˆ°æ•°æ®åº“...")
        
        try:
            # æ•°æ®åº“é…ç½®
            config = {
                'host': os.getenv('DB_HOST', 'localhost'),
                'database': os.getenv('DB_DATABASE', '90nihongo'),
                'user': os.getenv('DB_USERNAME', 'root'),
                'password': os.getenv('DB_PASSWORD', ''),
                'charset': 'utf8mb4'
            }
            
            connection = mysql.connector.connect(**config)
            cursor = connection.cursor()
            
            # åˆ›å»ºæµ‹è¯•ä»»åŠ¡
            cursor.execute("""
                INSERT INTO import_tasks (type, name, status, progress, total_items, items_processed, config, logs, created_at, updated_at)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, NOW(), NOW())
            """, (
                'web-scraping',
                'æµ‹è¯•æ–°NHKç»“æ„æŠ“å–',
                'running',
                0,
                len(scraped_articles),
                0,
                json.dumps({'urls': ['https://www3.nhk.or.jp/news/easy/']}),
                json.dumps(['å¼€å§‹æµ‹è¯•æŠ“å–'])
            ))
            
            task_id = cursor.lastrowid
            print(f"   åˆ›å»ºæµ‹è¯•ä»»åŠ¡ID: {task_id}")
            
            # ä¿å­˜æ–‡ç« åˆ°æ•°æ®åº“
            saved_count = 0
            for article in scraped_articles:
                try:
                    cursor.execute("""
                        INSERT INTO resource_items (name, type, source, content, status, metadata, created_at, updated_at)
                        VALUES (%s, %s, %s, %s, %s, %s, NOW(), NOW())
                    """, (
                        article['title'],
                        article['type'],
                        article['source'],
                        article['content'],
                        'completed',
                        json.dumps({
                            'url': article['url'],
                            'difficulty': 'easy',
                            'language': 'japanese'
                        })
                    ))
                    saved_count += 1
                    print(f"   âœ… ä¿å­˜æ–‡ç« : {article['title']}")
                    
                except Exception as e:
                    print(f"   âŒ ä¿å­˜å¤±è´¥: {e}")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            cursor.execute("""
                UPDATE import_tasks 
                SET status = %s, progress = %s, items_processed = %s, logs = %s, updated_at = NOW()
                WHERE id = %s
            """, (
                'completed',
                100,
                saved_count,
                json.dumps(['æµ‹è¯•æŠ“å–å®Œæˆ', f'æˆåŠŸä¿å­˜{saved_count}ç¯‡æ–‡ç« ']),
                task_id
            ))
            
            connection.commit()
            print(f"   âœ… æˆåŠŸä¿å­˜ {saved_count} ç¯‡æ–‡ç« åˆ°æ•°æ®åº“")
            
            # éªŒè¯ä¿å­˜ç»“æœ
            cursor.execute("SELECT COUNT(*) FROM resource_items")
            total_resources = cursor.fetchone()[0]
            
            cursor.execute("SELECT name, LENGTH(content) as content_length FROM resource_items ORDER BY created_at DESC LIMIT 3")
            recent_resources = cursor.fetchall()
            
            print(f"\nğŸ“š æ•°æ®åº“éªŒè¯:")
            print(f"   æ€»èµ„æºæ•°é‡: {total_resources}")
            print(f"   æœ€æ–°èµ„æº:")
            for name, length in recent_resources:
                print(f"     - {name} ({length} å­—ç¬¦)")
            
            cursor.close()
            connection.close()
            
        except Exception as e:
            print(f"âŒ æ•°æ®åº“æ“ä½œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    print("\nğŸ¯ ç»“è®º:")
    if scraped_articles:
        print("âœ… NHK Easy NewsæŠ“å–åŠŸèƒ½æ­£å¸¸å·¥ä½œ")
        print("âœ… æ•°æ®åº“ä¿å­˜åŠŸèƒ½æ­£å¸¸")
        print("ğŸ’¡ ä¹‹å‰100%å®Œæˆä½†æ— å†…å®¹çš„é—®é¢˜æ˜¯å› ä¸º:")
        print("   1. ç½‘ç«™ç»“æ„å·²æ”¹å˜ï¼Œæ—§çš„é“¾æ¥æ¨¡å¼å¤±æ•ˆ")
        print("   2. éœ€è¦æ›´æ–°web_scraper.pyä¸­çš„é“¾æ¥æ£€æµ‹é€»è¾‘")
    else:
        print("âŒ æŠ“å–åŠŸèƒ½ä»æœ‰é—®é¢˜")

except Exception as e:
    print(f"âŒ æ€»ä½“é”™è¯¯: {e}")
    import traceback
    traceback.print_exc() 