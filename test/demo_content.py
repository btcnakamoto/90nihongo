#!/usr/bin/env python3
"""
90å¤©æ—¥è¯­å­¦ä¹ å¹³å° - å†…å®¹è·å–æ¼”ç¤ºè„šæœ¬
ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®å±•ç¤ºå®Œæ•´çš„å†…å®¹è·å–å’Œå¯¼å…¥æµç¨‹
"""

import json
from pathlib import Path
from datetime import datetime
import csv

def print_banner():
    """æ‰“å°å¯åŠ¨æ¨ªå¹…"""
    print("ğŸŒ" * 30)
    print("90å¤©æ—¥è¯­å­¦ä¹ å¹³å°")
    print("å†…å®¹è·å–ä¸å¯¼å…¥ - å®Œæ•´æ¼”ç¤º")
    print("ğŸŒ" * 30)
    print()

def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹æ—¥è¯­å­¦ä¹ æ•°æ®"""
    print("ğŸ“Š ç”Ÿæˆç¤ºä¾‹æ—¥è¯­å­¦ä¹ å†…å®¹...")
    
    # æ¨¡æ‹ŸNHK Easyæ–°é—»å†…å®¹
    sample_articles = [
        {
            "title": "æ–°ã—ã„å…¬åœ’ãŒå®Œæˆã—ã¾ã—ãŸ",
            "content": "æ±äº¬ã«æ–°ã—ã„å…¬åœ’ãŒå®Œæˆã—ã¾ã—ãŸã€‚ã“ã®å…¬åœ’ã«ã¯ç¾ã—ã„èŠ±ã‚„æœ¨ãŒãŸãã•ã‚“ã‚ã‚Šã¾ã™ã€‚å®¶æ—ã§æ¥½ã—ã‚ã‚‹å ´æ‰€ã§ã™ã€‚æ¯æ—¥å¤šãã®äººãŒæ•£æ­©ã‚„é‹å‹•ã‚’ã—ã¦ã„ã¾ã™ã€‚å…¬åœ’ã®ä¸­ã«ã¯å­ä¾›ã®éŠã³å ´ã‚‚ã‚ã‚Šã¾ã™ã€‚",
            "audio_filename": "park_news.mp3",
            "difficulty": "beginner",
            "source": "NHK Easy",
            "category": "news"
        },
        {
            "title": "æ—¥æœ¬ã®ä¼çµ±çš„ãªç¥­ã‚Š",
            "content": "äº¬éƒ½ã§ä¼çµ±çš„ãªç¥­ã‚ŠãŒè¡Œã‚ã‚Œã¾ã—ãŸã€‚ç¾ã—ã„ç€ç‰©ã‚’ç€ãŸäººã€…ãŒè¡—ã‚’æ­©ãã¾ã—ãŸã€‚å¤ªé¼“ã®éŸ³ã¨è¸Šã‚ŠãŒã¨ã¦ã‚‚ç´ æ™´ã‚‰ã—ã‹ã£ãŸã§ã™ã€‚è¦³å…‰å®¢ã‚‚ãŸãã•ã‚“æ¥ã¦ã€å†™çœŸã‚’æ’®ã£ã¦ã„ã¾ã—ãŸã€‚",
            "audio_filename": "festival_news.mp3",
            "difficulty": "intermediate",
            "source": "NHK Easy",
            "category": "culture"
        },
        {
            "title": "æ–°ã—ã„é›»è»Šã®è·¯ç·š",
            "content": "æ¥æœˆã€æ–°ã—ã„é›»è»Šã®è·¯ç·šãŒé–‹é€šã—ã¾ã™ã€‚ã“ã®è·¯ç·šã¯éƒ½å¸‚ã®ä¸­å¿ƒéƒ¨ã¨éƒŠå¤–ã‚’çµã³ã¾ã™ã€‚é€šå‹¤ãŒã¨ã¦ã‚‚ä¾¿åˆ©ã«ãªã‚Šã¾ã™ã€‚ç’°å¢ƒã«ã‚‚å„ªã—ã„é›»è»Šã§ã™ã€‚",
            "audio_filename": "train_news.mp3",
            "difficulty": "intermediate",
            "source": "NHK Easy",
            "category": "transportation"
        },
        {
            "title": "å­£ç¯€ã®æ–™ç†æ•™å®¤",
            "content": "åœ°åŸŸã‚»ãƒ³ã‚¿ãƒ¼ã§æ–™ç†æ•™å®¤ãŒé–‹ã‹ã‚Œã¾ã™ã€‚å­£ç¯€ã®é‡èœã‚’ä½¿ã£ãŸå¥åº·çš„ãªæ–™ç†ã‚’å­¦ã¹ã¾ã™ã€‚åˆå¿ƒè€…ã®æ–¹ã‚‚æ­“è¿ã§ã™ã€‚ç¾å‘³ã—ã„æ–™ç†ã‚’ä¸€ç·’ã«ä½œã‚Šã¾ã—ã‚‡ã†ã€‚",
            "audio_filename": "cooking_class.mp3",
            "difficulty": "beginner",
            "source": "NHK Easy",
            "category": "lifestyle"
        },
        {
            "title": "å­¦æ ¡ã§æ–°ã—ã„æˆæ¥­ãŒå§‹ã¾ã‚Šã¾ã™",
            "content": "å°å­¦æ ¡ã§ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®æˆæ¥­ãŒå§‹ã¾ã‚Šã¾ã—ãŸã€‚å­ä¾›ãŸã¡ã¯ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ã£ã¦ç°¡å˜ãªã‚²ãƒ¼ãƒ ã‚’ä½œã‚Šã¾ã™ã€‚æ–°ã—ã„æŠ€è¡“ã‚’å­¦ã¶ã“ã¨ã¯å°†æ¥ã®ãŸã‚ã«å¤§åˆ‡ã§ã™ã€‚",
            "audio_filename": "programming_class.mp3",
            "difficulty": "intermediate",
            "source": "NHK Easy",
            "category": "education"
        }
    ]
    
    # æ¨¡æ‹Ÿè¯æ±‡æ•°æ®
    sample_vocabulary = [
        {"word": "å…¬åœ’", "reading": "ã“ã†ãˆã‚“", "meaning": "å…¬å›­", "jlpt_level": "N5", "audio": "kouen.mp3"},
        {"word": "å®Œæˆ", "reading": "ã‹ã‚“ã›ã„", "meaning": "å®Œæˆ", "jlpt_level": "N3", "audio": "kansei.mp3"},
        {"word": "ç¾ã—ã„", "reading": "ã†ã¤ãã—ã„", "meaning": "ç¾ä¸½çš„", "jlpt_level": "N4", "audio": "utsukushii.mp3"},
        {"word": "ä¼çµ±", "reading": "ã§ã‚“ã¨ã†", "meaning": "ä¼ ç»Ÿ", "jlpt_level": "N3", "audio": "dentou.mp3"},
        {"word": "ç¥­ã‚Š", "reading": "ã¾ã¤ã‚Š", "meaning": "èŠ‚æ—¥", "jlpt_level": "N4", "audio": "matsuri.mp3"},
        {"word": "ç€ç‰©", "reading": "ãã‚‚ã®", "meaning": "å’Œæœ", "jlpt_level": "N4", "audio": "kimono.mp3"},
        {"word": "é›»è»Š", "reading": "ã§ã‚“ã—ã‚ƒ", "meaning": "ç”µè½¦", "jlpt_level": "N5", "audio": "densha.mp3"},
        {"word": "è·¯ç·š", "reading": "ã‚ã›ã‚“", "meaning": "è·¯çº¿", "jlpt_level": "N3", "audio": "rosen.mp3"},
        {"word": "æ–™ç†", "reading": "ã‚Šã‚‡ã†ã‚Š", "meaning": "æ–™ç†", "jlpt_level": "N5", "audio": "ryouri.mp3"},
        {"word": "é‡èœ", "reading": "ã‚„ã•ã„", "meaning": "è”¬èœ", "jlpt_level": "N5", "audio": "yasai.mp3"}
    ]
    
    print(f"âœ… ç”Ÿæˆäº† {len(sample_articles)} ç¯‡æ–‡ç« ")
    print(f"âœ… ç”Ÿæˆäº† {len(sample_vocabulary)} ä¸ªè¯æ±‡")
    
    return sample_articles, sample_vocabulary

def create_csv_files(articles, vocabulary):
    """åˆ›å»ºCSVå¯¼å…¥æ–‡ä»¶"""
    print("\nğŸ“ åˆ›å»ºCSVå¯¼å…¥æ–‡ä»¶...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("demo_import_data")
    output_dir.mkdir(exist_ok=True)
    
    # 1. åˆ›å»ºè¯¾ç¨‹CSV
    courses_file = output_dir / "courses.csv"
    with open(courses_file, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['title', 'description', 'day_number', 'difficulty', 'tags', 'is_active', 'content', 'audio_file', 'source'])
        
        for i, article in enumerate(articles, 1):
            writer.writerow([
                article['title'],
                f"ç¬¬{i}å¤©è¯¾ç¨‹ï¼š{article['title']}",
                i,
                article['difficulty'],
                f'["æ—¥è¯­å­¦ä¹ ", "{article["category"]}"]',
                'true',
                article['content'],
                article['audio_filename'],
                article['source']
            ])
    
    print(f"âœ… è¯¾ç¨‹CSVæ–‡ä»¶å·²åˆ›å»º: {courses_file}")
    
    # 2. åˆ›å»ºå­¦ä¹ ææ–™CSV
    materials_file = output_dir / "materials.csv"
    with open(materials_file, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['course_id', 'title', 'type', 'content', 'media_url', 'duration_minutes', 'metadata', 'audio_file'])
        
        for i, article in enumerate(articles, 1):
            writer.writerow([
                i,
                f"{article['title']} - å­¦ä¹ ææ–™",
                'audio',
                article['content'],
                article['audio_filename'],
                5,  # ä¼°ç®—5åˆ†é’Ÿ
                json.dumps({"source": article['source'], "category": article['category']}, ensure_ascii=False),
                article['audio_filename']
            ])
    
    print(f"âœ… å­¦ä¹ ææ–™CSVæ–‡ä»¶å·²åˆ›å»º: {materials_file}")
    
    # 3. åˆ›å»ºè¯æ±‡CSV
    vocabulary_file = output_dir / "vocabulary.csv"
    with open(vocabulary_file, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['word', 'reading', 'meaning', 'part_of_speech', 'example_sentence', 'jlpt_level', 'tags', 'audio_file'])
        
        for vocab in vocabulary:
            writer.writerow([
                vocab['word'],
                vocab['reading'],
                vocab['meaning'],
                'åè¯',  # ç®€åŒ–å¤„ç†
                f"{vocab['word']}ã‚’ä½¿ã£ãŸä¾‹æ–‡ã§ã™ã€‚",
                vocab['jlpt_level'],
                '["åŸºç¡€è¯æ±‡", "æ—¥å¸¸ç”¨è¯­"]',
                vocab['audio']
            ])
    
    print(f"âœ… è¯æ±‡CSVæ–‡ä»¶å·²åˆ›å»º: {vocabulary_file}")
    
    return {
        'courses': str(courses_file),
        'materials': str(materials_file),
        'vocabulary': str(vocabulary_file)
    }

def create_audio_mapping(articles, vocabulary):
    """åˆ›å»ºéŸ³é¢‘æ–‡ä»¶æ˜ å°„"""
    print("\nğŸµ åˆ›å»ºéŸ³é¢‘æ–‡ä»¶æ˜ å°„...")
    
    audio_files = []
    
    # æ–‡ç« éŸ³é¢‘
    for i, article in enumerate(articles, 1):
        audio_files.append({
            "filename": article['audio_filename'],
            "content_type": "course",
            "content_id": i,
            "title": article['title'],
            "match_confidence": 1.0,
            "match_reason": "ç²¾ç¡®æ–‡ä»¶ååŒ¹é…"
        })
    
    # è¯æ±‡éŸ³é¢‘
    for vocab in vocabulary:
        audio_files.append({
            "filename": vocab['audio'],
            "content_type": "vocabulary",
            "content_word": vocab['word'],
            "title": vocab['word'],
            "match_confidence": 1.0,
            "match_reason": "è¯æ±‡å‘éŸ³åŒ¹é…"
        })
    
    # ä¿å­˜éŸ³é¢‘æ˜ å°„æ–‡ä»¶
    audio_mapping_file = Path("demo_import_data") / "audio_mapping.json"
    with open(audio_mapping_file, 'w', encoding='utf-8') as f:
        json.dump(audio_files, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… éŸ³é¢‘æ˜ å°„æ–‡ä»¶å·²åˆ›å»º: {audio_mapping_file}")
    print(f"ğŸ“Š æ€»è®¡ {len(audio_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶éœ€è¦å…³è”")
    
    return audio_files

def create_import_report(csv_files, audio_files, articles, vocabulary):
    """åˆ›å»ºå¯¼å…¥æŠ¥å‘Š"""
    print("\nğŸ“‹ ç”Ÿæˆå¯¼å…¥æŠ¥å‘Š...")
    
    report = {
        "timestamp": datetime.now().isoformat(),
        "summary": {
            "total_articles": len(articles),
            "total_vocabulary": len(vocabulary),
            "total_audio_files": len(audio_files),
            "csv_files_created": len(csv_files)
        },
        "csv_files": csv_files,
        "audio_mapping_preview": audio_files[:5],  # å‰5ä¸ªä½œä¸ºé¢„è§ˆ
        "import_steps": [
            "1. ç™»å½•ç½‘ç«™ç®¡ç†åå°",
            "2. è¿›å…¥'å†…å®¹ç®¡ç†'é¡µé¢",
            "3. ç‚¹å‡»'æ‰¹é‡å¯¼å…¥'æŒ‰é’®",
            "4. ä¸Šä¼ courses.csvæ–‡ä»¶ï¼Œé€‰æ‹©'è¯¾ç¨‹'ç±»å‹",
            "5. ä¸Šä¼ materials.csvæ–‡ä»¶ï¼Œé€‰æ‹©'å­¦ä¹ ææ–™'ç±»å‹",
            "6. ä¸Šä¼ vocabulary.csvæ–‡ä»¶ï¼Œé€‰æ‹©'è¯æ±‡'ç±»å‹",
            "7. è¿›å…¥'éŸ³é¢‘ç®¡ç†'ä¸Šä¼ å¯¹åº”çš„éŸ³é¢‘æ–‡ä»¶",
            "8. ä½¿ç”¨'æ™ºèƒ½å…³è”'åŠŸèƒ½è‡ªåŠ¨åŒ¹é…éŸ³é¢‘ä¸å†…å®¹",
            "9. å®¡æ ¸å¹¶ç¡®è®¤æ‰€æœ‰å…³è”å…³ç³»",
            "10. æµ‹è¯•å­¦ä¹ æ•ˆæœå¹¶è°ƒæ•´å†…å®¹"
        ],
        "expected_results": {
            "courses_created": len(articles),
            "materials_created": len(articles),
            "vocabulary_created": len(vocabulary),
            "audio_associations": len(audio_files),
            "learning_days_covered": len(articles)
        },
        "quality_metrics": {
            "beginner_content": len([a for a in articles if a['difficulty'] == 'beginner']),
            "intermediate_content": len([a for a in articles if a['difficulty'] == 'intermediate']),
            "n5_vocabulary": len([v for v in vocabulary if v['jlpt_level'] == 'N5']),
            "n4_vocabulary": len([v for v in vocabulary if v['jlpt_level'] == 'N4']),
            "n3_vocabulary": len([v for v in vocabulary if v['jlpt_level'] == 'N3'])
        }
    }
    
    report_file = Path("demo_import_data") / "import_report.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å¯¼å…¥æŠ¥å‘Šå·²åˆ›å»º: {report_file}")
    return report

def print_summary(report):
    """æ‰“å°æ€»ç»“ä¿¡æ¯"""
    print("\n" + "ğŸ‰" * 40)
    print("å†…å®¹è·å–ä¸å¯¼å…¥æ¼”ç¤ºå®Œæˆ!")
    print("ğŸ‰" * 40)
    
    print(f"\nğŸ“Š å†…å®¹ç»Ÿè®¡:")
    print(f"   ğŸ“š æ–‡ç« æ•°é‡: {report['summary']['total_articles']}")
    print(f"   ğŸ“– è¯æ±‡æ•°é‡: {report['summary']['total_vocabulary']}")
    print(f"   ğŸµ éŸ³é¢‘æ–‡ä»¶: {report['summary']['total_audio_files']}")
    print(f"   ğŸ“ CSVæ–‡ä»¶: {report['summary']['csv_files_created']}")
    
    print(f"\nğŸ“ˆ è´¨é‡åˆ†æ:")
    print(f"   ğŸŸ¢ åˆçº§å†…å®¹: {report['quality_metrics']['beginner_content']} ç¯‡")
    print(f"   ğŸŸ¡ ä¸­çº§å†…å®¹: {report['quality_metrics']['intermediate_content']} ç¯‡")
    print(f"   ğŸ“˜ N5è¯æ±‡: {report['quality_metrics']['n5_vocabulary']} ä¸ª")
    print(f"   ğŸ“— N4è¯æ±‡: {report['quality_metrics']['n4_vocabulary']} ä¸ª")
    print(f"   ğŸ“™ N3è¯æ±‡: {report['quality_metrics']['n3_vocabulary']} ä¸ª")
    
    print(f"\nğŸ“‚ ç”Ÿæˆçš„æ–‡ä»¶:")
    for file_type, file_path in report['csv_files'].items():
        print(f"   ğŸ“„ {file_type}: {file_path}")
    
    print(f"\nğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ:")
    for i, step in enumerate(report['import_steps'][:5], 1):
        print(f"   {i}. {step}")
    
    print("\nğŸ’¡ é‡è¦æç¤º:")
    print("   ğŸ”¸ è¿™æ˜¯æ¼”ç¤ºæ•°æ®ï¼Œå®é™…ä½¿ç”¨æ—¶è¯·è·å–çœŸå®å†…å®¹")
    print("   ğŸ”¸ éŸ³é¢‘æ–‡ä»¶éœ€è¦å•ç‹¬å‡†å¤‡æˆ–é€šè¿‡TTSç”Ÿæˆ")
    print("   ğŸ”¸ å»ºè®®å…ˆå°è§„æ¨¡æµ‹è¯•å¯¼å…¥æµç¨‹")
    print("   ğŸ”¸ ä½¿ç”¨æ™ºèƒ½å…³è”åŠŸèƒ½å¯å¤§å¤§æé«˜æ•ˆç‡")

def simulate_real_workflow():
    """æ¨¡æ‹ŸçœŸå®çš„å·¥ä½œæµç¨‹"""
    print("\n" + "=" * 50)
    print("æ¨¡æ‹ŸçœŸå®å†…å®¹è·å–å·¥ä½œæµç¨‹")
    print("=" * 50)
    
    workflows = [
        {
            "name": "æ–¹æ¡ˆA: NHK Easy (æ¨è)",
            "description": "ä½¿ç”¨NHK Easy Japaneseå…è´¹å†…å®¹",
            "steps": [
                "è¿è¡Œ python scraper.py --nhk-limit 30",
                "è·å–30ç¯‡é«˜è´¨é‡æ–°é—»æ–‡ç« ",
                "è‡ªåŠ¨ä¸‹è½½å¯¹åº”çš„éŸ³é¢‘æ–‡ä»¶", 
                "è¿è¡Œ python content_importer.py",
                "ç”Ÿæˆæ ‡å‡†CSVæ–‡ä»¶",
                "ä½¿ç”¨ç½‘ç«™æ‰¹é‡å¯¼å…¥åŠŸèƒ½"
            ],
            "cost": "å…è´¹",
            "time": "2-4å°æ—¶",
            "quality": "â­â­â­â­â­"
        },
        {
            "name": "æ–¹æ¡ˆB: æ··åˆå†…å®¹",
            "description": "ç»“åˆå¤šä¸ªæ•°æ®æº",
            "steps": [
                "NHK Easy (50ç¯‡æ–‡ç« )",
                "Forvo API (è¯æ±‡å‘éŸ³)",
                "è‡ªåˆ¶å†…å®¹ (ç‰¹å®šä¸»é¢˜)",
                "åˆå¹¶å¤„ç†å’Œè´¨é‡æ§åˆ¶",
                "æ‰¹é‡å¯¼å…¥å’ŒéŸ³é¢‘å…³è”"
            ],
            "cost": "$0-30",
            "time": "1-2å¤©",
            "quality": "â­â­â­â­â­"
        },
        {
            "name": "æ–¹æ¡ˆC: AIç”Ÿæˆå†…å®¹",
            "description": "ä½¿ç”¨AIç”Ÿæˆå®šåˆ¶åŒ–å†…å®¹",
            "steps": [
                "ä½¿ç”¨ChatGPTç”Ÿæˆæ—¥è¯­å¯¹è¯",
                "Google TTSç”ŸæˆéŸ³é¢‘",
                "äººå·¥å®¡æ ¸å’Œè°ƒæ•´",
                "æ ‡å‡†åŒ–æ ¼å¼å¤„ç†",
                "å¯¼å…¥åˆ°å­¦ä¹ å¹³å°"
            ],
            "cost": "$10-50",
            "time": "3-5å¤©",
            "quality": "â­â­â­â­"
        }
    ]
    
    for i, workflow in enumerate(workflows, 1):
        print(f"\nğŸ¯ {workflow['name']}")
        print(f"   ğŸ“ æè¿°: {workflow['description']}")
        print(f"   ğŸ’° æˆæœ¬: {workflow['cost']}")
        print(f"   â° æ—¶é—´: {workflow['time']}")
        print(f"   ğŸŒŸ è´¨é‡: {workflow['quality']}")
        print("   ğŸ“‹ æ­¥éª¤:")
        for j, step in enumerate(workflow['steps'], 1):
            print(f"      {j}. {step}")

def main():
    """ä¸»å‡½æ•°"""
    print_banner()
    
    # 1. ç”Ÿæˆç¤ºä¾‹æ•°æ®
    articles, vocabulary = create_sample_data()
    
    # 2. åˆ›å»ºCSVæ–‡ä»¶
    csv_files = create_csv_files(articles, vocabulary)
    
    # 3. åˆ›å»ºéŸ³é¢‘æ˜ å°„
    audio_files = create_audio_mapping(articles, vocabulary)
    
    # 4. ç”ŸæˆæŠ¥å‘Š
    report = create_import_report(csv_files, audio_files, articles, vocabulary)
    
    # 5. æ‰“å°æ€»ç»“
    print_summary(report)
    
    # 6. æ¨¡æ‹ŸçœŸå®å·¥ä½œæµç¨‹
    simulate_real_workflow()
    
    print("\nğŸš€ æ¼”ç¤ºå®Œæˆ!")
    print("ç°åœ¨æ‚¨å·²ç»äº†è§£äº†å®Œæ•´çš„å†…å®¹è·å–å’Œå¯¼å…¥æµç¨‹ã€‚")
    print("å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ–¹æ¡ˆå¼€å§‹å®æ–½ã€‚")

if __name__ == "__main__":
    main() 