#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ€ç»ˆç‰ˆç½‘é¡µæŠ“å–å™¨æµ‹è¯• - æ¨¡æ‹ŸLaravelè°ƒç”¨
"""

import sys
import json
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from pathlib import Path
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('final_scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class FinalWebScraper:
    """æœ€ç»ˆç‰ˆç½‘é¡µæŠ“å–å™¨"""
    
    def __init__(self, config):
        self.config = config
        self.task_id = config.get('task_id', 1)
        self.session = requests.Session()
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })
        
        # å­˜å‚¨æŠ“å–ç»“æœ
        self.scraped_items = []
        
    def find_nhk_articles(self, base_url, max_articles=10):
        """æŸ¥æ‰¾NHK Easy Newsæ–‡ç« é“¾æ¥"""
        try:
            logger.info(f"è®¿é—®NHKä¸»é¡µ: {base_url}")
            response = self.session.get(base_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            article_links = []
            
            # æŸ¥æ‰¾articleé“¾æ¥
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                if href and 'article' in href:
                    full_url = urljoin(base_url, href)
                    article_links.append(full_url)
            
            # å»é‡å¹¶é™åˆ¶æ•°é‡
            article_links = list(set(article_links))[:max_articles]
            logger.info(f"æ‰¾åˆ° {len(article_links)} ä¸ªæ–‡ç« é“¾æ¥")
            
            return article_links
            
        except Exception as e:
            logger.error(f"æŸ¥æ‰¾æ–‡ç« é“¾æ¥å¤±è´¥: {e}")
            return []
    
    def scrape_single_article(self, url):
        """æŠ“å–å•ç¯‡æ–‡ç« """
        try:
            logger.info(f"æŠ“å–æ–‡ç« : {url}")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æå–æ ‡é¢˜
            title = soup.find('title')
            title_text = title.get_text(strip=True) if title else "æœªçŸ¥æ ‡é¢˜"
            
            # æå–å†…å®¹ - å°è¯•å¤šç§é€‰æ‹©å™¨
            content = ""
            content_selectors = [
                'div[id*="article"]',
                'div[class*="article"]', 
                'div[class*="content"]',
                'main',
                'article',
                '.content'
            ]
            
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    content = ' '.join([elem.get_text(strip=True) for elem in elements])
                    if len(content) > 100:
                        break
            
            # å¦‚æœè¿˜æ²¡æœ‰æ‰¾åˆ°å†…å®¹ï¼Œæå–body
            if len(content) < 50:
                body = soup.find('body')
                if body:
                    content = body.get_text(strip=True)
            
            # æå–å›¾ç‰‡
            images = []
            for img in soup.find_all('img'):
                src = img.get('src')
                if src and not src.startswith('data:'):
                    full_url = urljoin(url, src)
                    images.append(full_url)
            
            # æå–éŸ³é¢‘
            audio_urls = []
            for audio in soup.find_all(['audio', 'source']):
                src = audio.get('src')
                if src and ('.mp3' in src or '.wav' in src):
                    full_url = urljoin(url, src)
                    audio_urls.append(full_url)
            
            result = {
                'url': url,
                'title': title_text,
                'content': content,
                'content_length': len(content),
                'images': images[:3],
                'audio': audio_urls[:2],
                'metadata': {
                    'source': 'NHK Easy News',
                    'difficulty': 'easy',
                    'language': 'japanese',
                    'scraped_at': time.strftime('%Y-%m-%d %H:%M:%S')
                }
            }
            
            # åªä¿å­˜æœ‰æ•ˆå†…å®¹çš„æ–‡ç« 
            if len(content) > 100:
                logger.info(f"æˆåŠŸæŠ“å–: {title_text[:30]}... (å†…å®¹: {len(content)} å­—ç¬¦)")
                return result
            else:
                logger.warning(f"å†…å®¹å¤ªçŸ­ï¼Œè·³è¿‡: {title_text[:30]}...")
                return None
            
        except Exception as e:
            logger.error(f"æŠ“å–æ–‡ç« å¤±è´¥ {url}: {e}")
            return None
    
    def simulate_database_save(self, article_data):
        """æ¨¡æ‹Ÿä¿å­˜åˆ°æ•°æ®åº“"""
        # è¿™é‡Œæ¨¡æ‹ŸLaravelä¼šåšçš„æ•°æ®åº“ä¿å­˜æ“ä½œ
        save_data = {
            'name': article_data['title'],
            'type': self.config.get('content_type', 'material'),
            'source': 'web-scraping',
            'source_url': article_data['url'],
            'content': article_data['content'],
            'metadata': json.dumps(article_data['metadata'], ensure_ascii=False),
            'media_files': json.dumps({
                'images': article_data['images'],
                'audio': article_data['audio']
            }, ensure_ascii=False),
            'status': 'completed',
            'progress': 100,
            'import_task_id': self.task_id
        }
        
        # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
        self.scraped_items.append(save_data)
        logger.info(f"æ¨¡æ‹Ÿä¿å­˜æˆåŠŸ: {article_data['title'][:30]}...")
    
    def run_scraping_job(self):
        """è¿è¡ŒæŠ“å–ä»»åŠ¡"""
        try:
            urls = self.config.get('urls', [])
            max_pages = self.config.get('max_pages', 10)
            delay_ms = self.config.get('delay_ms', 1000)
            
            logger.info(f"å¼€å§‹æŠ“å–ä»»åŠ¡ï¼Œç›®æ ‡URLs: {len(urls)}, æœ€å¤§é¡µé¢: {max_pages}")
            
            total_processed = 0
            
            for base_url in urls:
                logger.info(f"å¤„ç†URL: {base_url}")
                
                if 'nhk.or.jp/news/easy' in base_url:
                    # è·å–æ–‡ç« é“¾æ¥
                    article_links = self.find_nhk_articles(base_url, max_pages)
                    
                    # æŠ“å–æ¯ç¯‡æ–‡ç« 
                    for article_url in article_links:
                        if total_processed >= max_pages:
                            break
                            
                        article_data = self.scrape_single_article(article_url)
                        
                        if article_data:
                            self.simulate_database_save(article_data)
                            total_processed += 1
                        
                        # å»¶è¿Ÿ
                        if delay_ms > 0:
                            time.sleep(delay_ms / 1000)
                else:
                    # ç›´æ¥æŠ“å–URL
                    article_data = self.scrape_single_article(base_url)
                    if article_data:
                        self.simulate_database_save(article_data)
                        total_processed += 1
            
            # ä¿å­˜ç»“æœ
            output_file = 'final_scraping_results.json'
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(self.scraped_items, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æŠ“å–ä»»åŠ¡å®Œæˆï¼å…±å¤„ç† {total_processed} ç¯‡æ–‡ç« ")
            logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            
            # æ‰“å°æ‘˜è¦æŠ¥å‘Š
            print(f"\nğŸ‰ æŠ“å–ä»»åŠ¡å®Œæˆæ‘˜è¦:")
            print(f"ğŸ“„ æˆåŠŸæŠ“å–æ–‡ç« æ•°: {len(self.scraped_items)}")
            print(f"ğŸ“Š å¹³å‡å†…å®¹é•¿åº¦: {sum(len(item['content']) for item in self.scraped_items) // len(self.scraped_items) if self.scraped_items else 0} å­—ç¬¦")
            
            for i, item in enumerate(self.scraped_items, 1):
                print(f"\n{i}. {item['name'][:50]}...")
                print(f"   ğŸ“ å†…å®¹é•¿åº¦: {len(item['content'])} å­—ç¬¦")
                print(f"   ğŸ”— æ¥æº: {item['source_url']}")
            
            return len(self.scraped_items)
            
        except Exception as e:
            logger.error(f"æŠ“å–ä»»åŠ¡å¤±è´¥: {e}")
            return 0

def main():
    """ä¸»å‡½æ•° - å¯ä»¥æ¥å—é…ç½®æ–‡ä»¶å‚æ•°ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨é»˜è®¤é…ç½®"""
    
    # é»˜è®¤é…ç½®
    default_config = {
        'task_id': 999,
        'urls': ['https://www3.nhk.or.jp/news/easy/'],
        'max_pages': 5,
        'content_type': 'course',
        'delay_ms': 1500,
        'include_images': False,
        'include_audio': False
    }
    
    # å¦‚æœæä¾›äº†é…ç½®æ–‡ä»¶ï¼Œåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶
    if len(sys.argv) > 1:
        config_file = sys.argv[1]
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logger.info(f"ä½¿ç”¨é…ç½®æ–‡ä»¶: {config_file}")
        except Exception as e:
            logger.error(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            config = default_config
    else:
        config = default_config
        logger.info("ä½¿ç”¨é»˜è®¤é…ç½®")
    
    print("ğŸš€ å¯åŠ¨æœ€ç»ˆç‰ˆç½‘é¡µæŠ“å–å™¨...")
    print(f"ğŸ“‹ é…ç½®: {json.dumps(config, ensure_ascii=False, indent=2)}")
    
    # è¿è¡ŒæŠ“å–
    scraper = FinalWebScraper(config)
    result_count = scraper.run_scraping_job()
    
    if result_count > 0:
        print(f"\nâœ… ä»»åŠ¡æˆåŠŸå®Œæˆï¼å…±æŠ“å– {result_count} ç¯‡æ–‡ç« ")
        print("ğŸ’¡ è¿™è¯´æ˜ç½‘é¡µæŠ“å–åŠŸèƒ½å®Œå…¨æ­£å¸¸ï¼Œå¯ä»¥é›†æˆåˆ°Laravelç³»ç»Ÿä¸­")
    else:
        print(f"\nâŒ ä»»åŠ¡å¤±è´¥ï¼Œæ²¡æœ‰æŠ“å–åˆ°ä»»ä½•å†…å®¹")

if __name__ == "__main__":
    main() 