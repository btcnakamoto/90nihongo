#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•ç½‘é¡µæŠ“å–åŠŸèƒ½
"""

import json
import tempfile
from web_scraper import JapaneseWebScraper

def test_scraper():
    """æµ‹è¯•æŠ“å–å™¨åŠŸèƒ½"""
    
    # æ¨¡æ‹Ÿé…ç½®
    config = {
        'task_id': 1,
        'urls': [
            'https://www3.nhk.or.jp/news/easy/',
            # å¯ä»¥æ·»åŠ æ›´å¤šæµ‹è¯•URL
        ],
        'max_pages': 2,
        'content_type': 'course',
        'delay_ms': 1000,
        'include_images': False,  # æµ‹è¯•æ—¶ä¸ä¸‹è½½å›¾ç‰‡
        'include_audio': False,   # æµ‹è¯•æ—¶ä¸ä¸‹è½½éŸ³é¢‘
        'database_config': {
            'host': 'localhost',
            'database': 'test_db',
            'username': 'test_user',
            'password': 'test_pass'
        }
    }
    
    print("ğŸš€ å¼€å§‹æµ‹è¯•ç½‘é¡µæŠ“å–åŠŸèƒ½...")
    
    try:
        # åˆ›å»ºæŠ“å–å™¨å®ä¾‹
        scraper = JapaneseWebScraper(config)
        
        # æµ‹è¯•å•ä¸ªURLå†…å®¹æå–
        test_url = 'https://www3.nhk.or.jp/news/easy/'
        print(f"ğŸ“° æµ‹è¯•URL: {test_url}")
        
        content = scraper.extract_content(test_url)
        
        if content:
            print("âœ… å†…å®¹æå–æˆåŠŸ!")
            print(f"ğŸ“ æ ‡é¢˜: {content['title'][:50]}...")
            print(f"ğŸ“„ å†…å®¹é•¿åº¦: {len(content['content'])} å­—ç¬¦")
            print(f"ğŸ–¼ï¸ å›¾ç‰‡æ•°é‡: {len(content['images'])}")
            print(f"ğŸ”Š éŸ³é¢‘æ•°é‡: {len(content['audio'])}")
            print(f"ğŸ“Š å…ƒæ•°æ®: {content['metadata']}")
            
            # ä¿å­˜æµ‹è¯•ç»“æœ
            with open('test_result.json', 'w', encoding='utf-8') as f:
                json.dump(content, f, ensure_ascii=False, indent=2)
            print("ğŸ“ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ° test_result.json")
            
        else:
            print("âŒ å†…å®¹æå–å¤±è´¥")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    test_scraper() 