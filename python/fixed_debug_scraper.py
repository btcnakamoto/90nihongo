#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®å¤ç‰ˆè°ƒè¯•ç½‘é¡µæŠ“å–å™¨
"""

import json
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('fixed_debug_scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class FixedDebugScraper:
    """ä¿®å¤ç‰ˆè°ƒè¯•æŠ“å–å™¨"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })
    
    def get_nhk_article_links(self, base_url):
        """è·å–NHK Easy Newsæ–‡ç« é“¾æ¥"""
        try:
            logger.info(f"è®¿é—®: {base_url}")
            response = self.session.get(base_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            article_links = []
            
            # æŸ¥æ‰¾æ–‡ç« é“¾æ¥
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                if href and 'article' in href:
                    full_url = urljoin(base_url, href)
                    article_links.append(full_url)
            
            # å»é‡
            article_links = list(set(article_links))
            logger.info(f"æ‰¾åˆ° {len(article_links)} ä¸ªæ–‡ç« é“¾æ¥")
            
            return article_links
            
        except Exception as e:
            logger.error(f"è·å–æ–‡ç« é“¾æ¥å¤±è´¥: {e}")
            return []
    
    def scrape_article(self, url):
        """æŠ“å–å•ç¯‡æ–‡ç« """
        try:
            logger.info(f"æŠ“å–æ–‡ç« : {url}")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æå–æ ‡é¢˜
            title = soup.find('title')
            title_text = title.get_text(strip=True) if title else ""
            
            # å°è¯•æå–æ­£æ–‡å†…å®¹
            content = ""
            
            # å°è¯•å¤šç§å†…å®¹é€‰æ‹©å™¨
            content_selectors = [
                'div[id*="article"]',
                'div[class*="article"]', 
                'div[class*="content"]',
                'div[class*="body"]',
                'main',
                'article',
                '.content',
                '#content'
            ]
            
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    content = ' '.join([elem.get_text(strip=True) for elem in elements])
                    if len(content) > 100:  # è‡³å°‘è¦æœ‰ä¸€å®šé•¿åº¦çš„å†…å®¹
                        break
            
            # å¦‚æœè¿˜æ²¡æœ‰æ‰¾åˆ°å†…å®¹ï¼Œæå–bodyæ–‡æœ¬
            if len(content) < 50:
                body = soup.find('body')
                if body:
                    content = body.get_text(strip=True)
            
            # æå–å›¾ç‰‡
            images = []
            for img in soup.find_all('img'):
                src = img.get('src')
                if src and not src.startswith('data:'):
                    full_url = urljoin(url, src)
                    images.append(full_url)
            
            result = {
                'url': url,
                'title': title_text,
                'content': content,
                'content_length': len(content),
                'images': images[:3],
                'metadata': {
                    'source': 'NHK Easy News',
                    'difficulty': 'easy',
                    'language': 'japanese'
                }
            }
            
            logger.info(f"æˆåŠŸæŠ“å–: {title_text[:30]}... (å†…å®¹: {len(content)} å­—ç¬¦)")
            return result
            
        except Exception as e:
            logger.error(f"æŠ“å–æ–‡ç« å¤±è´¥ {url}: {e}")
            return None
    
    def run_scraping_test(self):
        """è¿è¡ŒæŠ“å–æµ‹è¯•"""
        base_url = 'https://www3.nhk.or.jp/news/easy/'
        
        print("ğŸš€ å¼€å§‹æŠ“å–æµ‹è¯•...")
        
        # è·å–æ–‡ç« é“¾æ¥
        article_links = self.get_nhk_article_links(base_url)
        
        if not article_links:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ–‡ç« é“¾æ¥")
            return []
        
        # æŠ“å–å‰3ç¯‡æ–‡ç« 
        results = []
        for i, url in enumerate(article_links[:3]):
            print(f"\nğŸ“° æŠ“å–ç¬¬ {i+1} ç¯‡æ–‡ç« ...")
            article = self.scrape_article(url)
            if article:
                results.append(article)
            time.sleep(1)  # å»¶è¿Ÿ1ç§’
        
        # ä¿å­˜ç»“æœ
        output_file = 'fixed_scraping_results.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°æ‘˜è¦
        print(f"\nğŸ“Š æŠ“å–ç»“æœæ‘˜è¦:")
        print(f"ğŸ“„ æ€»æ–‡ç« æ•°: {len(results)}")
        
        for i, article in enumerate(results, 1):
            print(f"\n{i}. {article['title'][:50]}...")
            print(f"   ğŸ”— URL: {article['url']}")
            print(f"   ğŸ“ å†…å®¹é•¿åº¦: {article['content_length']} å­—ç¬¦")
            print(f"   ğŸ–¼ï¸ å›¾ç‰‡: {len(article['images'])} å¼ ")
            if article['content_length'] > 0:
                print(f"   ğŸ“– å†…å®¹é¢„è§ˆ: {article['content'][:100]}...")
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        return results

def main():
    scraper = FixedDebugScraper()
    results = scraper.run_scraping_test()
    
    if results:
        print(f"\nâœ… æŠ“å–æˆåŠŸï¼å…±è·å– {len(results)} ç¯‡æ–‡ç« ")
    else:
        print(f"\nâŒ æŠ“å–å¤±è´¥ï¼Œæ²¡æœ‰è·å–åˆ°ä»»ä½•å†…å®¹")

if __name__ == "__main__":
    main() 