import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

print("ğŸ” åˆ†æNHK Easy Newsç½‘ç«™ç»“æ„...")

try:
    response = requests.get('https://www3.nhk.or.jp/news/easy/', timeout=30)
    print(f"çŠ¶æ€ç : {response.status_code}")
    
    soup = BeautifulSoup(response.content, 'html.parser')
    
    print("\nğŸ“‹ é¡µé¢ä¸­çš„æ‰€æœ‰é“¾æ¥ç±»å‹:")
    all_links = soup.find_all('a', href=True)
    link_patterns = {}
    
    for link in all_links:
        href = link.get('href')
        if href:
            # åˆ†æé“¾æ¥æ¨¡å¼
            if '/news/easy/' in href:
                pattern = href.split('/news/easy/')[-1].split('/')[0] if '/' in href.split('/news/easy/')[-1] else href.split('/news/easy/')[-1]
                if pattern not in link_patterns:
                    link_patterns[pattern] = []
                link_patterns[pattern].append(href)
    
    print("å‘ç°çš„é“¾æ¥æ¨¡å¼:")
    for pattern, links in link_patterns.items():
        print(f"  {pattern}: {len(links)} ä¸ªé“¾æ¥")
        if len(links) <= 3:
            for link in links:
                print(f"    - {link}")
        else:
            for link in links[:3]:
                print(f"    - {link}")
            print(f"    ... è¿˜æœ‰ {len(links)-3} ä¸ª")
    
    print("\nğŸ” æŸ¥æ‰¾å¯èƒ½çš„æ–‡ç« é“¾æ¥:")
    
    # å°è¯•ä¸åŒçš„æ–‡ç« é“¾æ¥æ¨¡å¼
    patterns_to_check = [
        '/news/easy/article/',
        '/news/easy/k10',
        '/easy/',
        'article',
        'k10'
    ]
    
    for pattern in patterns_to_check:
        matching_links = []
        for link in all_links:
            href = link.get('href')
            if href and pattern in href:
                full_url = urljoin('https://www3.nhk.or.jp/news/easy/', href)
                matching_links.append(full_url)
        
        if matching_links:
            print(f"\næ¨¡å¼ '{pattern}' æ‰¾åˆ° {len(matching_links)} ä¸ªé“¾æ¥:")
            for i, link in enumerate(matching_links[:5]):
                print(f"  {i+1}. {link}")
            if len(matching_links) > 5:
                print(f"  ... è¿˜æœ‰ {len(matching_links)-5} ä¸ª")
    
    print("\nğŸ“„ é¡µé¢æ ‡é¢˜å’Œä¸»è¦å†…å®¹:")
    title = soup.find('title')
    if title:
        print(f"é¡µé¢æ ‡é¢˜: {title.get_text(strip=True)}")
    
    # æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
    main_content = soup.find('main') or soup.find('div', class_='main') or soup.find('div', id='main')
    if main_content:
        print(f"ä¸»è¦å†…å®¹åŒºåŸŸé•¿åº¦: {len(main_content.get_text(strip=True))} å­—ç¬¦")
        
        # åœ¨ä¸»è¦å†…å®¹ä¸­æŸ¥æ‰¾é“¾æ¥
        main_links = main_content.find_all('a', href=True)
        print(f"ä¸»è¦å†…å®¹ä¸­çš„é“¾æ¥æ•°é‡: {len(main_links)}")
        
        article_links_in_main = []
        for link in main_links:
            href = link.get('href')
            text = link.get_text(strip=True)
            if href and ('article' in href or 'k10' in href or len(text) > 10):
                article_links_in_main.append((href, text[:50]))
        
        if article_links_in_main:
            print("ä¸»è¦å†…å®¹ä¸­å¯èƒ½çš„æ–‡ç« é“¾æ¥:")
            for href, text in article_links_in_main[:10]:
                print(f"  - {href} -> {text}")
    
    print("\nğŸ·ï¸  æŸ¥æ‰¾å…·æœ‰ç‰¹å®šclassæˆ–idçš„å…ƒç´ :")
    
    # æŸ¥æ‰¾å¯èƒ½åŒ…å«æ–‡ç« åˆ—è¡¨çš„å…ƒç´ 
    potential_containers = [
        soup.find_all('div', class_=lambda x: x and ('article' in x.lower() or 'news' in x.lower() or 'list' in x.lower())),
        soup.find_all('ul', class_=lambda x: x and ('article' in x.lower() or 'news' in x.lower() or 'list' in x.lower())),
        soup.find_all('section', class_=lambda x: x and ('article' in x.lower() or 'news' in x.lower() or 'list' in x.lower()))
    ]
    
    for container_type, containers in zip(['div', 'ul', 'section'], potential_containers):
        if containers:
            print(f"\n{container_type} å®¹å™¨:")
            for container in containers[:3]:
                class_name = container.get('class', [])
                id_name = container.get('id', '')
                links_in_container = container.find_all('a', href=True)
                print(f"  class='{class_name}' id='{id_name}' - {len(links_in_container)} ä¸ªé“¾æ¥")
                
                for link in links_in_container[:3]:
                    href = link.get('href')
                    text = link.get_text(strip=True)[:30]
                    print(f"    - {href} -> {text}")

except Exception as e:
    print(f"âŒ é”™è¯¯: {e}")
    import traceback
    traceback.print_exc() 