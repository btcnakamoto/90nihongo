#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿæµ‹è¯•NHK Easy NewsæŠ“å–åŠŸèƒ½
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import json
import time

def test_nhk_scraping():
    print("ğŸ” å¿«é€Ÿæµ‹è¯•NHK Easy NewsæŠ“å–...")
    
    # æµ‹è¯•ä¸»é¡µè®¿é—®
    base_url = "https://www3.nhk.or.jp/news/easy/"
    
    try:
        print(f"ğŸ“¡ è®¿é—®ä¸»é¡µ: {base_url}")
        response = requests.get(base_url, timeout=30)
        response.raise_for_status()
        print(f"âœ… ä¸»é¡µè®¿é—®æˆåŠŸï¼ŒçŠ¶æ€ç : {response.status_code}")
        
        # è§£æé¡µé¢
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æŸ¥æ‰¾æ–‡ç« é“¾æ¥
        article_links = []
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if href and '/news/easy/article/' in href:
                full_url = urljoin(base_url, href)
                article_links.append(full_url)
        
        # å»é‡
        article_links = list(set(article_links))
        print(f"ğŸ“° æ‰¾åˆ° {len(article_links)} ä¸ªæ–‡ç« é“¾æ¥")
        
        if len(article_links) == 0:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ–‡ç« é“¾æ¥ï¼æ£€æŸ¥ç½‘ç«™ç»“æ„...")
            # è¾“å‡ºé¡µé¢ä¸­çš„æ‰€æœ‰é“¾æ¥è¿›è¡Œè°ƒè¯•
            print("ğŸ” é¡µé¢ä¸­çš„æ‰€æœ‰é“¾æ¥:")
            all_links = soup.find_all('a', href=True)[:10]  # åªæ˜¾ç¤ºå‰10ä¸ª
            for i, link in enumerate(all_links):
                href = link.get('href')
                text = link.get_text(strip=True)[:50]
                print(f"   {i+1}. {href} -> {text}")
            return False
        
        # æµ‹è¯•æŠ“å–ç¬¬ä¸€ç¯‡æ–‡ç« 
        test_url = article_links[0]
        print(f"ğŸ“– æµ‹è¯•æŠ“å–æ–‡ç« : {test_url}")
        
        article_response = requests.get(test_url, timeout=30)
        article_response.raise_for_status()
        
        article_soup = BeautifulSoup(article_response.content, 'html.parser')
        
        # æå–æ ‡é¢˜
        title = article_soup.find('title')
        title_text = title.get_text(strip=True) if title else "æœªçŸ¥æ ‡é¢˜"
        
        # æå–å†…å®¹
        content = ""
        content_selectors = [
            'div[id*="article"]',
            'div[class*="article"]', 
            'div[class*="content"]',
            'main',
            'article',
            '.content'
        ]
        
        for selector in content_selectors:
            elements = article_soup.select(selector)
            if elements:
                content = ' '.join([elem.get_text(strip=True) for elem in elements])
                if len(content) > 100:
                    break
        
        # å¦‚æœè¿˜æ²¡æœ‰æ‰¾åˆ°å†…å®¹ï¼Œæå–body
        if len(content) < 50:
            body = article_soup.find('body')
            if body:
                content = body.get_text(strip=True)
        
        print(f"ğŸ“ æ–‡ç« æ ‡é¢˜: {title_text}")
        print(f"ğŸ“„ å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        print(f"ğŸ“„ å†…å®¹é¢„è§ˆ: {content[:200]}...")
        
        if len(content) > 100:
            print("âœ… æŠ“å–æµ‹è¯•æˆåŠŸï¼")
            return True
        else:
            print("âŒ æŠ“å–çš„å†…å®¹å¤ªå°‘ï¼Œå¯èƒ½æœ‰é—®é¢˜")
            return False
            
    except Exception as e:
        print(f"âŒ æŠ“å–æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_database_connection():
    print("\nğŸ” æµ‹è¯•æ•°æ®åº“è¿æ¥...")
    try:
        import mysql.connector
        import os
        from dotenv import load_dotenv
        
        # åŠ è½½ç¯å¢ƒå˜é‡
        load_dotenv('backend/.env')
        
        # æ•°æ®åº“é…ç½®
        config = {
            'host': os.getenv('DB_HOST', 'localhost'),
            'database': os.getenv('DB_DATABASE', '90nihongo'),
            'user': os.getenv('DB_USERNAME', 'root'),
            'password': os.getenv('DB_PASSWORD', ''),
            'charset': 'utf8mb4'
        }
        
        print(f"ğŸ“¡ è¿æ¥æ•°æ®åº“: {config['host']}/{config['database']}")
        
        connection = mysql.connector.connect(**config)
        cursor = connection.cursor()
        
        # æ£€æŸ¥ä»»åŠ¡è¡¨
        cursor.execute("SELECT COUNT(*) FROM import_tasks WHERE status = 'completed'")
        completed_tasks = cursor.fetchone()[0]
        
        # æ£€æŸ¥èµ„æºè¡¨
        cursor.execute("SELECT COUNT(*) FROM resource_items")
        resource_count = cursor.fetchone()[0]
        
        print(f"ğŸ“Š å·²å®Œæˆä»»åŠ¡: {completed_tasks} ä¸ª")
        print(f"ğŸ“š èµ„æºæ€»æ•°: {resource_count} ä¸ª")
        
        if resource_count == 0:
            print("âŒ æ•°æ®åº“ä¸­æ²¡æœ‰èµ„æºå†…å®¹ï¼è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆä»»åŠ¡å®Œæˆä½†æ²¡æœ‰æ˜¾ç¤ºç»“æœ")
        else:
            # æ˜¾ç¤ºæœ€æ–°èµ„æº
            cursor.execute("""
                SELECT name, LENGTH(content) as content_length, created_at 
                FROM resource_items 
                ORDER BY created_at DESC 
                LIMIT 3
            """)
            resources = cursor.fetchall()
            print("ğŸ“– æœ€æ–°èµ„æº:")
            for name, length, created_at in resources:
                print(f"   - {name} ({length} å­—ç¬¦) - {created_at}")
        
        cursor.close()
        connection.close()
        print("âœ… æ•°æ®åº“è¿æ¥æµ‹è¯•æˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹å¿«é€Ÿè¯Šæ–­...")
    print("=" * 50)
    
    # æµ‹è¯•ç½‘ç«™æŠ“å–
    scraping_ok = test_nhk_scraping()
    
    # æµ‹è¯•æ•°æ®åº“
    db_ok = test_database_connection()
    
    print("\n" + "=" * 50)
    print("ğŸ¯ è¯Šæ–­ç»“æœ:")
    
    if scraping_ok and db_ok:
        print("âœ… æŠ“å–åŠŸèƒ½æ­£å¸¸ï¼Œæ•°æ®åº“è¿æ¥æ­£å¸¸")
        print("ğŸ’¡ é—®é¢˜å¯èƒ½åœ¨äº:")
        print("   1. é˜Ÿåˆ—å·¥ä½œè¿›ç¨‹æ²¡æœ‰è¿è¡Œ")
        print("   2. Laravelä»»åŠ¡è°ƒç”¨Pythonè„šæœ¬æ—¶å‡ºé”™")
        print("   3. å‰ç«¯APIæ²¡æœ‰æ­£ç¡®è¯»å–æ•°æ®")
    elif scraping_ok and not db_ok:
        print("âš ï¸  æŠ“å–åŠŸèƒ½æ­£å¸¸ä½†æ•°æ®åº“æœ‰é—®é¢˜")
        print("ğŸ’¡ éœ€è¦æ£€æŸ¥æ•°æ®åº“é…ç½®å’Œè¿æ¥")
    elif not scraping_ok and db_ok:
        print("âš ï¸  æ•°æ®åº“æ­£å¸¸ä½†æŠ“å–æœ‰é—®é¢˜")
        print("ğŸ’¡ éœ€è¦æ£€æŸ¥ç½‘ç«™ç»“æ„å˜åŒ–æˆ–ç½‘ç»œè¿æ¥")
    else:
        print("âŒ æŠ“å–å’Œæ•°æ®åº“éƒ½æœ‰é—®é¢˜")
        print("ğŸ’¡ éœ€è¦å…¨é¢æ£€æŸ¥é…ç½®å’Œç¯å¢ƒ") 